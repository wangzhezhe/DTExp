### Typical patterns

The simulation code comes from the https://github.com/pnorbert/adiosvm.

One dimension to distinguish the patterns according to the strategies of the data checking service. (The place that executes the data checking service)

The typical dynamic task trigger patterns evaluated in this project contains:

Producer-responsible: the data checking service is integrated with the data producer such as the simulation.

Consumer-responsible: the data checking service is integrated with the data consumer such as the analytics.

Middleware-responsible: the data checking service is integrated with the middleware such as the data management service.

Notification-responsible: the data checking service is integrated with the middleware and the information of the qualified data is acquired by the event notification.

The time sequence figures are listed as follows:

Producer-responsible:

<img src="./img/umlprc.png" alt="drawing" width="300" title="Producer-responsible">

Consumer-responsible:

<img src="./img/umlcrc.png" alt="drawing" width="300" title="Consumer-responsible">

Middleware-responsible:

<img src="./img/umlmcn.png" alt="drawing" width="300" title="Middleware-responsible">

Another dimension to distinguish different patterns is how to start the data analytics. The typical pattern is to start one type of tasks in one program. This program may process the different data from the different steps. Another pattern is to start the program of analytics for one or several units of the data. The basic unit is the data generated by simulation in specific step.

There are following combinations if we put two dimensions together. We assume the staging techniques are used here which means the simulation and the analytics does need to process in step alternately, the data transfered between task is based on parallel I/O.

| the place of checking data and the method of starting analytics | one program for one type of analytics       | one program for one data unit   |
|----------------------------|----------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| simulation                 | increasing the time of the simulation there is minimal data transfer between tasks                             | increasing the concurrency between the execution of the simulation and analytics  start task when it is necessary |
| middleware                 | increasing the execution time between simulation and data checking                                             | same as above                                                                                                     |
| consumer                   | same as above. Besides, the task might idle for a long time  if the percentage of the interesting data is low  | Non, because the data checking is binded together with the analytic's code             

### The parameters evaluated in this project

For the task setting, we compare the two cases. The first case is that the simulation is the bottleneck of the workflow execution. The second case is that the simulation execution is not the bottleneck of the workflow execution.

For the data size, we modify the number of the grid generated at each step of the simulation.

We also evaluate how the percentage of interesting things influence the workflow execution for different patterns.

The distribution of interesting things is also evaluated. The interesting things are evenly distributed during the workflow execution or distributed by a centralized pattern.

The frequency of data checking is also evaluated.


### The commands to compile the project on amarel cluster

```
module use /projects/community/modulefiles/
module load gcc/5.4/openmpi/3.1.2-kholodvl
cmake ~/cworkspace/src/DTExp/ -DADIOS2_DIR=~/cworkspace/build/build_ADIOS2/ -DVTK_DIR=~/cworkspace/build/build_vtk -DUSE_TIMERS=ON -DVTK=ON
```